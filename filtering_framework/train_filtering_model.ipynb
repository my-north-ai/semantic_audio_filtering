{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tmsantos/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from omegaconf import OmegaConf\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from logger import Logger\n",
    "from model_utils import merge_conf\n",
    "from trainer import MusCALLTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_conf_path = 'configs/base_config.yaml'\n",
    "dataset_conf_path = 'configs/dataset.yaml'\n",
    "model_conf_path = 'configs/model.yaml'\n",
    "muscall_config = merge_conf(base_conf_path=base_conf_path, dataset_conf_path=dataset_conf_path,model_conf_path=model_conf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(muscall_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "{'dataset_name': 'common_voice', 'data_dir': 'data/', 'train_filename': 'common_voice_16_1_train_manifest.json', 'val_filename': 'common_voice_16_1_train_manifest.json', 'text': {'max_seq_length': 256, 'tokenizer': 'albertinatokenizer'}, 'audio': {'sr': 16000, 'crop_length': 30, 'random_crop': True, 'augment': False, 'p_noise': 0.3, 'p_pitch_shift': 0.4}}\n",
      "data/common_voice_16_1_train_manifest.json\n",
      "{'dataset_name': 'common_voice', 'data_dir': 'data/', 'train_filename': 'common_voice_16_1_train_manifest.json', 'val_filename': 'common_voice_16_1_train_manifest.json', 'text': {'max_seq_length': 256, 'tokenizer': 'albertinatokenizer'}, 'audio': {'sr': 16000, 'crop_length': 30, 'random_crop': True, 'augment': False, 'p_noise': 0.3, 'p_pitch_shift': 0.4}}\n",
      "data/common_voice_16_1_train_manifest.json\n",
      "Number of training samples: 9414\n",
      "Building model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at PORTULAN/albertina-100m-portuguese-ptpt-encoder and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_scale True\n",
      "audio_backbone.encoder.conv1.weight False\n",
      "audio_backbone.encoder.conv1.bias False\n",
      "audio_backbone.encoder.conv2.weight False\n",
      "audio_backbone.encoder.conv2.bias False\n",
      "audio_backbone.encoder.embed_positions.weight False\n",
      "audio_backbone.encoder.layers.0.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.0.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.0.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.0.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.0.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.0.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.0.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.0.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.0.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.0.fc1.weight False\n",
      "audio_backbone.encoder.layers.0.fc1.bias False\n",
      "audio_backbone.encoder.layers.0.fc2.weight False\n",
      "audio_backbone.encoder.layers.0.fc2.bias False\n",
      "audio_backbone.encoder.layers.0.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.0.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.1.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.1.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.1.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.1.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.1.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.1.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.1.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.1.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.1.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.1.fc1.weight False\n",
      "audio_backbone.encoder.layers.1.fc1.bias False\n",
      "audio_backbone.encoder.layers.1.fc2.weight False\n",
      "audio_backbone.encoder.layers.1.fc2.bias False\n",
      "audio_backbone.encoder.layers.1.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.1.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.2.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.2.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.2.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.2.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.2.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.2.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.2.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.2.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.2.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.2.fc1.weight False\n",
      "audio_backbone.encoder.layers.2.fc1.bias False\n",
      "audio_backbone.encoder.layers.2.fc2.weight False\n",
      "audio_backbone.encoder.layers.2.fc2.bias False\n",
      "audio_backbone.encoder.layers.2.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.2.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.3.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.3.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.3.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.3.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.3.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.3.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.3.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.3.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.3.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.3.fc1.weight False\n",
      "audio_backbone.encoder.layers.3.fc1.bias False\n",
      "audio_backbone.encoder.layers.3.fc2.weight False\n",
      "audio_backbone.encoder.layers.3.fc2.bias False\n",
      "audio_backbone.encoder.layers.3.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.3.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.4.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.4.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.4.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.4.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.4.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.4.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.4.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.4.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.4.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.4.fc1.weight False\n",
      "audio_backbone.encoder.layers.4.fc1.bias False\n",
      "audio_backbone.encoder.layers.4.fc2.weight False\n",
      "audio_backbone.encoder.layers.4.fc2.bias False\n",
      "audio_backbone.encoder.layers.4.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.4.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.5.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.5.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.5.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.5.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.5.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.5.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.5.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.5.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.5.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.5.fc1.weight False\n",
      "audio_backbone.encoder.layers.5.fc1.bias False\n",
      "audio_backbone.encoder.layers.5.fc2.weight False\n",
      "audio_backbone.encoder.layers.5.fc2.bias False\n",
      "audio_backbone.encoder.layers.5.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.5.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.6.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.6.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.6.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.6.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.6.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.6.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.6.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.6.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.6.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.6.fc1.weight False\n",
      "audio_backbone.encoder.layers.6.fc1.bias False\n",
      "audio_backbone.encoder.layers.6.fc2.weight False\n",
      "audio_backbone.encoder.layers.6.fc2.bias False\n",
      "audio_backbone.encoder.layers.6.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.6.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.7.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.7.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.7.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.7.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.7.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.7.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.7.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.7.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.7.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.7.fc1.weight False\n",
      "audio_backbone.encoder.layers.7.fc1.bias False\n",
      "audio_backbone.encoder.layers.7.fc2.weight False\n",
      "audio_backbone.encoder.layers.7.fc2.bias False\n",
      "audio_backbone.encoder.layers.7.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.7.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.8.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.8.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.8.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.8.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.8.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.8.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.8.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.8.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.8.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.8.fc1.weight False\n",
      "audio_backbone.encoder.layers.8.fc1.bias False\n",
      "audio_backbone.encoder.layers.8.fc2.weight False\n",
      "audio_backbone.encoder.layers.8.fc2.bias False\n",
      "audio_backbone.encoder.layers.8.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.8.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.9.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.9.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.9.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.9.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.9.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.9.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.9.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.9.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.9.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.9.fc1.weight False\n",
      "audio_backbone.encoder.layers.9.fc1.bias False\n",
      "audio_backbone.encoder.layers.9.fc2.weight False\n",
      "audio_backbone.encoder.layers.9.fc2.bias False\n",
      "audio_backbone.encoder.layers.9.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.9.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.10.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.10.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.10.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.10.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.10.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.10.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.10.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.10.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.10.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.10.fc1.weight False\n",
      "audio_backbone.encoder.layers.10.fc1.bias False\n",
      "audio_backbone.encoder.layers.10.fc2.weight False\n",
      "audio_backbone.encoder.layers.10.fc2.bias False\n",
      "audio_backbone.encoder.layers.10.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.10.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.11.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.11.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.11.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.11.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.11.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.11.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.11.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.11.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.11.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.11.fc1.weight False\n",
      "audio_backbone.encoder.layers.11.fc1.bias False\n",
      "audio_backbone.encoder.layers.11.fc2.weight False\n",
      "audio_backbone.encoder.layers.11.fc2.bias False\n",
      "audio_backbone.encoder.layers.11.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.11.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.12.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.12.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.12.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.12.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.12.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.12.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.12.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.12.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.12.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.12.fc1.weight False\n",
      "audio_backbone.encoder.layers.12.fc1.bias False\n",
      "audio_backbone.encoder.layers.12.fc2.weight False\n",
      "audio_backbone.encoder.layers.12.fc2.bias False\n",
      "audio_backbone.encoder.layers.12.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.12.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.13.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.13.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.13.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.13.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.13.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.13.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.13.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.13.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.13.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.13.fc1.weight False\n",
      "audio_backbone.encoder.layers.13.fc1.bias False\n",
      "audio_backbone.encoder.layers.13.fc2.weight False\n",
      "audio_backbone.encoder.layers.13.fc2.bias False\n",
      "audio_backbone.encoder.layers.13.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.13.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.14.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.14.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.14.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.14.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.14.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.14.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.14.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.14.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.14.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.14.fc1.weight False\n",
      "audio_backbone.encoder.layers.14.fc1.bias False\n",
      "audio_backbone.encoder.layers.14.fc2.weight False\n",
      "audio_backbone.encoder.layers.14.fc2.bias False\n",
      "audio_backbone.encoder.layers.14.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.14.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.15.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.15.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.15.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.15.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.15.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.15.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.15.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.15.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.15.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.15.fc1.weight False\n",
      "audio_backbone.encoder.layers.15.fc1.bias False\n",
      "audio_backbone.encoder.layers.15.fc2.weight False\n",
      "audio_backbone.encoder.layers.15.fc2.bias False\n",
      "audio_backbone.encoder.layers.15.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.15.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.16.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.16.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.16.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.16.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.16.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.16.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.16.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.16.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.16.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.16.fc1.weight False\n",
      "audio_backbone.encoder.layers.16.fc1.bias False\n",
      "audio_backbone.encoder.layers.16.fc2.weight False\n",
      "audio_backbone.encoder.layers.16.fc2.bias False\n",
      "audio_backbone.encoder.layers.16.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.16.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.17.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.17.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.17.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.17.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.17.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.17.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.17.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.17.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.17.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.17.fc1.weight False\n",
      "audio_backbone.encoder.layers.17.fc1.bias False\n",
      "audio_backbone.encoder.layers.17.fc2.weight False\n",
      "audio_backbone.encoder.layers.17.fc2.bias False\n",
      "audio_backbone.encoder.layers.17.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.17.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.18.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.18.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.18.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.18.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.18.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.18.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.18.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.18.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.18.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.18.fc1.weight False\n",
      "audio_backbone.encoder.layers.18.fc1.bias False\n",
      "audio_backbone.encoder.layers.18.fc2.weight False\n",
      "audio_backbone.encoder.layers.18.fc2.bias False\n",
      "audio_backbone.encoder.layers.18.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.18.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.19.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.19.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.19.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.19.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.19.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.19.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.19.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.19.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.19.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.19.fc1.weight False\n",
      "audio_backbone.encoder.layers.19.fc1.bias False\n",
      "audio_backbone.encoder.layers.19.fc2.weight False\n",
      "audio_backbone.encoder.layers.19.fc2.bias False\n",
      "audio_backbone.encoder.layers.19.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.19.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.20.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.20.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.20.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.20.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.20.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.20.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.20.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.20.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.20.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.20.fc1.weight False\n",
      "audio_backbone.encoder.layers.20.fc1.bias False\n",
      "audio_backbone.encoder.layers.20.fc2.weight False\n",
      "audio_backbone.encoder.layers.20.fc2.bias False\n",
      "audio_backbone.encoder.layers.20.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.20.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.21.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.21.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.21.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.21.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.21.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.21.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.21.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.21.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.21.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.21.fc1.weight False\n",
      "audio_backbone.encoder.layers.21.fc1.bias False\n",
      "audio_backbone.encoder.layers.21.fc2.weight False\n",
      "audio_backbone.encoder.layers.21.fc2.bias False\n",
      "audio_backbone.encoder.layers.21.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.21.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.22.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.22.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.22.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.22.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.22.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.22.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.22.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.22.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.22.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.22.fc1.weight False\n",
      "audio_backbone.encoder.layers.22.fc1.bias False\n",
      "audio_backbone.encoder.layers.22.fc2.weight False\n",
      "audio_backbone.encoder.layers.22.fc2.bias False\n",
      "audio_backbone.encoder.layers.22.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.22.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.23.self_attn.k_proj.weight False\n",
      "audio_backbone.encoder.layers.23.self_attn.v_proj.weight False\n",
      "audio_backbone.encoder.layers.23.self_attn.v_proj.bias False\n",
      "audio_backbone.encoder.layers.23.self_attn.q_proj.weight False\n",
      "audio_backbone.encoder.layers.23.self_attn.q_proj.bias False\n",
      "audio_backbone.encoder.layers.23.self_attn.out_proj.weight False\n",
      "audio_backbone.encoder.layers.23.self_attn.out_proj.bias False\n",
      "audio_backbone.encoder.layers.23.self_attn_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.23.self_attn_layer_norm.bias False\n",
      "audio_backbone.encoder.layers.23.fc1.weight False\n",
      "audio_backbone.encoder.layers.23.fc1.bias False\n",
      "audio_backbone.encoder.layers.23.fc2.weight False\n",
      "audio_backbone.encoder.layers.23.fc2.bias False\n",
      "audio_backbone.encoder.layers.23.final_layer_norm.weight False\n",
      "audio_backbone.encoder.layers.23.final_layer_norm.bias False\n",
      "audio_backbone.encoder.layer_norm.weight False\n",
      "audio_backbone.encoder.layer_norm.bias False\n",
      "textual_head.deberta.embeddings.word_embeddings.weight False\n",
      "textual_head.deberta.embeddings.LayerNorm.weight False\n",
      "textual_head.deberta.embeddings.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.0.attention.self.q_bias False\n",
      "textual_head.deberta.encoder.layer.0.attention.self.v_bias False\n",
      "textual_head.deberta.encoder.layer.0.attention.self.in_proj.weight False\n",
      "textual_head.deberta.encoder.layer.0.attention.self.pos_proj.weight False\n",
      "textual_head.deberta.encoder.layer.0.attention.self.pos_q_proj.weight False\n",
      "textual_head.deberta.encoder.layer.0.attention.self.pos_q_proj.bias False\n",
      "textual_head.deberta.encoder.layer.0.attention.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.0.attention.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.0.attention.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.0.attention.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.0.intermediate.dense.weight False\n",
      "textual_head.deberta.encoder.layer.0.intermediate.dense.bias False\n",
      "textual_head.deberta.encoder.layer.0.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.0.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.0.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.0.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.1.attention.self.q_bias False\n",
      "textual_head.deberta.encoder.layer.1.attention.self.v_bias False\n",
      "textual_head.deberta.encoder.layer.1.attention.self.in_proj.weight False\n",
      "textual_head.deberta.encoder.layer.1.attention.self.pos_proj.weight False\n",
      "textual_head.deberta.encoder.layer.1.attention.self.pos_q_proj.weight False\n",
      "textual_head.deberta.encoder.layer.1.attention.self.pos_q_proj.bias False\n",
      "textual_head.deberta.encoder.layer.1.attention.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.1.attention.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.1.attention.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.1.attention.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.1.intermediate.dense.weight False\n",
      "textual_head.deberta.encoder.layer.1.intermediate.dense.bias False\n",
      "textual_head.deberta.encoder.layer.1.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.1.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.1.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.1.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.2.attention.self.q_bias False\n",
      "textual_head.deberta.encoder.layer.2.attention.self.v_bias False\n",
      "textual_head.deberta.encoder.layer.2.attention.self.in_proj.weight False\n",
      "textual_head.deberta.encoder.layer.2.attention.self.pos_proj.weight False\n",
      "textual_head.deberta.encoder.layer.2.attention.self.pos_q_proj.weight False\n",
      "textual_head.deberta.encoder.layer.2.attention.self.pos_q_proj.bias False\n",
      "textual_head.deberta.encoder.layer.2.attention.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.2.attention.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.2.attention.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.2.attention.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.2.intermediate.dense.weight False\n",
      "textual_head.deberta.encoder.layer.2.intermediate.dense.bias False\n",
      "textual_head.deberta.encoder.layer.2.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.2.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.2.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.2.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.3.attention.self.q_bias False\n",
      "textual_head.deberta.encoder.layer.3.attention.self.v_bias False\n",
      "textual_head.deberta.encoder.layer.3.attention.self.in_proj.weight False\n",
      "textual_head.deberta.encoder.layer.3.attention.self.pos_proj.weight False\n",
      "textual_head.deberta.encoder.layer.3.attention.self.pos_q_proj.weight False\n",
      "textual_head.deberta.encoder.layer.3.attention.self.pos_q_proj.bias False\n",
      "textual_head.deberta.encoder.layer.3.attention.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.3.attention.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.3.attention.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.3.attention.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.3.intermediate.dense.weight False\n",
      "textual_head.deberta.encoder.layer.3.intermediate.dense.bias False\n",
      "textual_head.deberta.encoder.layer.3.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.3.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.3.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.3.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.4.attention.self.q_bias False\n",
      "textual_head.deberta.encoder.layer.4.attention.self.v_bias False\n",
      "textual_head.deberta.encoder.layer.4.attention.self.in_proj.weight False\n",
      "textual_head.deberta.encoder.layer.4.attention.self.pos_proj.weight False\n",
      "textual_head.deberta.encoder.layer.4.attention.self.pos_q_proj.weight False\n",
      "textual_head.deberta.encoder.layer.4.attention.self.pos_q_proj.bias False\n",
      "textual_head.deberta.encoder.layer.4.attention.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.4.attention.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.4.attention.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.4.attention.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.4.intermediate.dense.weight False\n",
      "textual_head.deberta.encoder.layer.4.intermediate.dense.bias False\n",
      "textual_head.deberta.encoder.layer.4.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.4.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.4.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.4.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.5.attention.self.q_bias False\n",
      "textual_head.deberta.encoder.layer.5.attention.self.v_bias False\n",
      "textual_head.deberta.encoder.layer.5.attention.self.in_proj.weight False\n",
      "textual_head.deberta.encoder.layer.5.attention.self.pos_proj.weight False\n",
      "textual_head.deberta.encoder.layer.5.attention.self.pos_q_proj.weight False\n",
      "textual_head.deberta.encoder.layer.5.attention.self.pos_q_proj.bias False\n",
      "textual_head.deberta.encoder.layer.5.attention.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.5.attention.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.5.attention.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.5.attention.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.5.intermediate.dense.weight False\n",
      "textual_head.deberta.encoder.layer.5.intermediate.dense.bias False\n",
      "textual_head.deberta.encoder.layer.5.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.5.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.5.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.5.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.6.attention.self.q_bias False\n",
      "textual_head.deberta.encoder.layer.6.attention.self.v_bias False\n",
      "textual_head.deberta.encoder.layer.6.attention.self.in_proj.weight False\n",
      "textual_head.deberta.encoder.layer.6.attention.self.pos_proj.weight False\n",
      "textual_head.deberta.encoder.layer.6.attention.self.pos_q_proj.weight False\n",
      "textual_head.deberta.encoder.layer.6.attention.self.pos_q_proj.bias False\n",
      "textual_head.deberta.encoder.layer.6.attention.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.6.attention.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.6.attention.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.6.attention.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.6.intermediate.dense.weight False\n",
      "textual_head.deberta.encoder.layer.6.intermediate.dense.bias False\n",
      "textual_head.deberta.encoder.layer.6.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.6.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.6.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.6.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.7.attention.self.q_bias False\n",
      "textual_head.deberta.encoder.layer.7.attention.self.v_bias False\n",
      "textual_head.deberta.encoder.layer.7.attention.self.in_proj.weight False\n",
      "textual_head.deberta.encoder.layer.7.attention.self.pos_proj.weight False\n",
      "textual_head.deberta.encoder.layer.7.attention.self.pos_q_proj.weight False\n",
      "textual_head.deberta.encoder.layer.7.attention.self.pos_q_proj.bias False\n",
      "textual_head.deberta.encoder.layer.7.attention.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.7.attention.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.7.attention.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.7.attention.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.7.intermediate.dense.weight False\n",
      "textual_head.deberta.encoder.layer.7.intermediate.dense.bias False\n",
      "textual_head.deberta.encoder.layer.7.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.7.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.7.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.7.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.8.attention.self.q_bias False\n",
      "textual_head.deberta.encoder.layer.8.attention.self.v_bias False\n",
      "textual_head.deberta.encoder.layer.8.attention.self.in_proj.weight False\n",
      "textual_head.deberta.encoder.layer.8.attention.self.pos_proj.weight False\n",
      "textual_head.deberta.encoder.layer.8.attention.self.pos_q_proj.weight False\n",
      "textual_head.deberta.encoder.layer.8.attention.self.pos_q_proj.bias False\n",
      "textual_head.deberta.encoder.layer.8.attention.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.8.attention.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.8.attention.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.8.attention.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.8.intermediate.dense.weight False\n",
      "textual_head.deberta.encoder.layer.8.intermediate.dense.bias False\n",
      "textual_head.deberta.encoder.layer.8.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.8.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.8.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.8.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.9.attention.self.q_bias False\n",
      "textual_head.deberta.encoder.layer.9.attention.self.v_bias False\n",
      "textual_head.deberta.encoder.layer.9.attention.self.in_proj.weight False\n",
      "textual_head.deberta.encoder.layer.9.attention.self.pos_proj.weight False\n",
      "textual_head.deberta.encoder.layer.9.attention.self.pos_q_proj.weight False\n",
      "textual_head.deberta.encoder.layer.9.attention.self.pos_q_proj.bias False\n",
      "textual_head.deberta.encoder.layer.9.attention.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.9.attention.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.9.attention.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.9.attention.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.9.intermediate.dense.weight False\n",
      "textual_head.deberta.encoder.layer.9.intermediate.dense.bias False\n",
      "textual_head.deberta.encoder.layer.9.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.9.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.9.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.9.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.10.attention.self.q_bias False\n",
      "textual_head.deberta.encoder.layer.10.attention.self.v_bias False\n",
      "textual_head.deberta.encoder.layer.10.attention.self.in_proj.weight False\n",
      "textual_head.deberta.encoder.layer.10.attention.self.pos_proj.weight False\n",
      "textual_head.deberta.encoder.layer.10.attention.self.pos_q_proj.weight False\n",
      "textual_head.deberta.encoder.layer.10.attention.self.pos_q_proj.bias False\n",
      "textual_head.deberta.encoder.layer.10.attention.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.10.attention.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.10.attention.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.10.attention.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.10.intermediate.dense.weight False\n",
      "textual_head.deberta.encoder.layer.10.intermediate.dense.bias False\n",
      "textual_head.deberta.encoder.layer.10.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.10.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.10.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.10.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.11.attention.self.q_bias False\n",
      "textual_head.deberta.encoder.layer.11.attention.self.v_bias False\n",
      "textual_head.deberta.encoder.layer.11.attention.self.in_proj.weight False\n",
      "textual_head.deberta.encoder.layer.11.attention.self.pos_proj.weight False\n",
      "textual_head.deberta.encoder.layer.11.attention.self.pos_q_proj.weight False\n",
      "textual_head.deberta.encoder.layer.11.attention.self.pos_q_proj.bias False\n",
      "textual_head.deberta.encoder.layer.11.attention.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.11.attention.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.11.attention.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.11.attention.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.layer.11.intermediate.dense.weight False\n",
      "textual_head.deberta.encoder.layer.11.intermediate.dense.bias False\n",
      "textual_head.deberta.encoder.layer.11.output.dense.weight False\n",
      "textual_head.deberta.encoder.layer.11.output.dense.bias False\n",
      "textual_head.deberta.encoder.layer.11.output.LayerNorm.weight False\n",
      "textual_head.deberta.encoder.layer.11.output.LayerNorm.bias False\n",
      "textual_head.deberta.encoder.rel_embeddings.weight False\n",
      "textual_head.pooler.dense.weight True\n",
      "textual_head.pooler.dense.bias True\n",
      "audio_projection.weight True\n",
      "text_projection.weight True\n",
      "Building optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tmsantos/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = MusCALLTrainer(muscall_config, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_path = os.path.join(muscall_config.env.experiments_dir, muscall_config.env.experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/15 17:40:42 INFO mlflow.tracking.fluent: Experiment with name '/Users/tmsantos/Documents/TTS_Augmentation/tts_data_augmentation/filtering_framework/save/experiments/model1' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/tmsantos/Documents/TTS_Augmentation/tts_data_augmentation/filtering_framework/mlruns/637985172241964419', creation_time=1721061642839, experiment_id='637985172241964419', last_update_time=1721061642839, lifecycle_stage='active', name='/Users/tmsantos/Documents/TTS_Augmentation/tts_data_augmentation/filtering_framework/save/experiments/model1', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "\n",
    "# Set the experiment name\n",
    "mlflow.set_experiment(experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training experiment with id model1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/294 [00:00<?, ?it/s]/Users/tmsantos/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Processing batches:   0%|          | 1/294 [01:01<4:58:55, 61.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start an MLflow run\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run():\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/TTS_Augmentation/tts_data_augmentation/filtering_framework/trainer.py:151\u001b[0m, in \u001b[0;36mMusCALLTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtraining\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m    149\u001b[0m     epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 151\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_train_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_loss, step\u001b[38;5;241m=\u001b[39mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    154\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_epoch_val(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loader)\n",
      "File \u001b[0;32m~/Documents/TTS_Augmentation/tts_data_augmentation/filtering_framework/trainer.py:198\u001b[0m, in \u001b[0;36mMusCALLTrainer.train_epoch\u001b[0;34m(self, data_loader, is_training)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Cast operations to mixed precision\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtraining\u001b[38;5;241m.\u001b[39mamp):\n\u001b[0;32m--> 198\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_mel_spectograms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_audio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_audio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msentence_sim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msentence_sim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_attention_mask\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_training:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtraining\u001b[38;5;241m.\u001b[39mamp:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/TTS_Augmentation/tts_data_augmentation/filtering_framework/model.py:127\u001b[0m, in \u001b[0;36mMusCALL.forward\u001b[0;34m(self, original_mel_spectograms, text, original_audio, sentence_sim, text_mask, return_loss)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_loss:\n\u001b[1;32m    124\u001b[0m     audio_ssl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 127\u001b[0m text_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m audio_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_audio(original_mel_spectograms)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# normalise features\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/TTS_Augmentation/tts_data_augmentation/filtering_framework/model.py:107\u001b[0m, in \u001b[0;36mMusCALL.encode_text\u001b[0;34m(self, text, text_mask)\u001b[0m\n\u001b[1;32m    105\u001b[0m     pooled_outout \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpooler_output\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtextual_head, DebertaForSequenceClassification):\n\u001b[0;32m--> 107\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextual_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     pooled_outout \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    110\u001b[0m text_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_projection(pooled_outout)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/TTS_Augmentation/tts_data_augmentation/filtering_framework/text_encoder.py:59\u001b[0m, in \u001b[0;36mDebertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m---> 59\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m encoder_layer \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     71\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(encoder_layer)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/transformers/models/deberta/modeling_deberta.py:964\u001b[0m, in \u001b[0;36mDebertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    954\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    956\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    957\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    958\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    961\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    962\u001b[0m )\n\u001b[0;32m--> 964\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/transformers/models/deberta/modeling_deberta.py:460\u001b[0m, in \u001b[0;36mDebertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    450\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    451\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    452\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    457\u001b[0m         output_attentions,\n\u001b[1;32m    458\u001b[0m     )\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 460\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    470\u001b[0m     hidden_states, att_m \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/transformers/models/deberta/modeling_deberta.py:373\u001b[0m, in \u001b[0;36mDebertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    366\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    371\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    372\u001b[0m ):\n\u001b[0;32m--> 373\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    382\u001b[0m         attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/transformers/models/deberta/modeling_deberta.py:306\u001b[0m, in \u001b[0;36mDebertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    299\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    305\u001b[0m ):\n\u001b[0;32m--> 306\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    315\u001b[0m         self_output, att_matrix \u001b[38;5;241m=\u001b[39m self_output\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/transformers/models/deberta/modeling_deberta.py:660\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtalking_head:\n\u001b[1;32m    658\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_logits_proj(attention_scores\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 660\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mXSoftmax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m(attention_scores, attention_mask, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    661\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_probs)\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtalking_head:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/tts-data-augmentation-vGKV77-n-py3.11/lib/python3.11/site-packages/torch/autograd/function.py:346\u001b[0m, in \u001b[0;36mFunctionMeta.__getattribute__\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    342\u001b[0m         _warn_traceable_deprecated()\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name, bases, attrs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, name):\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_traceable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    348\u001b[0m         _warn_traceable_deprecated()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts-data-augmentation-vGKV77-n-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
